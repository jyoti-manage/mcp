from typing import Any, Dict, List, Optional
from contextlib import AsyncExitStack

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from dotenv import load_dotenv
from openai import OpenAI
import json
from get_proxy_client import proxy_client

load_dotenv()

# Host can be any LLM or agents in most cases. In most cases, the clients are not needed to be implemented like this. 
# So, the clients are not implemented directly, instead the client's part or the connection, session etc. are implemented by most of the agent frameworks like langgraph, which is then just abstracted from us, so that we can simply use that instead of creating a client from scratch.
# For example, like a lot of sdk like Gemini are doing like they are connecting with mcp servers and getting the tools lists and giving then to agents (this is same as what we have done in this client.py, but this will be abstracted in their SDK) 

class MCPClient:
    def init_(self):
        # Initialize session and client objects
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack() # manage multiple asynchronous context managers and cleanup tasks. This ensures that all resources are properly cleaned up/closed at the end
        self.llm_model_name = 'gpt-40'
        self.openai_client = OpenAI(proxy_client=proxy_client)

    async def get_mcp_tools(self) -> List[Dict [str, Any]]:
        """Get available tools from the MCP server in OpenAI format.

        Returns:
            A list of tools in OpenAI format.
        """

        tools_result = await self.session.list_tools()

        return [
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema,
                },

            }
            for tool in tools_result.tools
        ]


    async def connect_to_server(self, server_script_path: str):

        """Connect to an MCP server

        Args:
            server_script_path: Path to the server script (.py)
        """

        command = "python"
        server_params = StdioServerParameters(
            command = command, 
            args=[server_script_path], 
            env=None
        ) # a class

        # context managers: (enter_async_context) Context managers automate this cleanup or closing. Used to connection with server
        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
        # create a client that communicates with a server over standard input and output (stdio)

        self.stdio, self.write = stdio_transport    # self.stdio is typically the read stream, and self.write is the write.

        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write)) # create a session object for communication with an MCP server

        await self.session.initialize()

        # List available tools
        tools = await self.get_mcp_tools()
        print("\nConnected to server with tools:", tools)


    #processing queries and handling tool calls
    async def process_query(self, query: str) -> str:
        """Process a query using Openai and available tools"""

        available_tools = await self.get_mcp_tools()

        messages = [
        {
            "role": "user",
            "content": query

        }]

        # Initial Claude API call
        # response = self.anthropic.messages.create(
        #     model="claude-3-5-sonnet-20241022",
        #     max_tokens=1000,
        #     messages=messages,
        #     tools = available_tools
        # )
        # If you include 'tools' in your API request, the model may return 'tool_use' content type that represent the model's use of those tools. You can then run those tools using the tool input generated by the model and then optionally return results back to the model using tool_result content blocks


        response = await self.openai_client.chat.completions.create(
             model=self.llm_model_name,
             messages = messages,
             tools = available_tools,
             tool_choice="auto",)

        # Process response and handle tool calls
        final_text = []

        # Get assistant's response
        assistant_message = response.choices[0].message

        while assistant_message.tool_calls:
            if assistant_message.content:
                final_text.append(assistant_message.content)

            messages.append(assistant_message)

            for tool_call in assistant_message.tool_calls:
                #Execute tool call
                result = await self.session.call_tool(
                    tool_call.function.name, 
                    arguments=json.loads(tool_call.function.arguments),
                )

                final_text.append(f"[Calling tool {tool_call.function.name} with args {json.loads(tool_call.function.arguments)}]")

                messages.append({
                    "role": "tool", 
                    "tool_call_id": tool_call.id, 
                    "content": result.content[0].text,
                })
            
            #Get final response from OpenAI with tool results
            response = await self.openai_client.chat.completions.create(
                model= self.llm_model_name,
                messages = messages, 
                tools = available_tools,
                tool_choice="auto",
            )
            assistant_message = response.choices[0].message

        if assistant_message.content:
            final_text.append(assistant_message.content)

        return "\n".join(final_text)
    
    async def chat_loop(self):
        """Run an interactive chat loop""" 
        print("\nMCP Client Started!") 
        print("Type your queries or 'quit' to exit.")

        while True:
            try:
                query = input("\nQuery: ").strip()
                
                if query.lower() == 'quit': 
                    break
                
                response = await self.process_query(query)
                print("\n" + response)

            except Exception as e: print(f"\nError: (str(e))")

    async def cleanup(self):
        """Clean up resources"""  
        await self.exit_stack.aclose()